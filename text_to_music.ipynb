{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEZYK4v9Ews2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import collections\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from typing import Optional\n",
        "import seaborn as sns\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install -y fluidsynth\n",
        "!pip install --upgrade pyfluidsynth\n",
        "import fluidsynth"
      ],
      "metadata": {
        "id": "oUUx7OauPm_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Unzip the music sentiment dataset\n",
        "(https://zenodo.org/record/5090631#.Y55keXbMJhF)"
      ],
      "metadata": {
        "id": "WQwW0nagG4fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://zenodo.org/record/5090631/files/EMOPIA_1.0.zip?download=1\"\n",
        "response = requests.get(URL)\n",
        "open(\"dataset.zip\", \"wb\").write(response.content)\n",
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "J98PF341G2XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dataset\n"
      ],
      "metadata": {
        "id": "SsvK_KviHozM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ANNOTATION_CSV = \"EMOPIA_1.0/label.csv\"\n",
        "AUDIO_DIRECTORY = \"EMOPIA_1.0/midis/*\""
      ],
      "metadata": {
        "id": "RLf8EywkH-NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "dataset = pd.read_csv(ANNOTATION_CSV)\n",
        "music_files = glob.glob(AUDIO_DIRECTORY)\n",
        "\n",
        "# get the unique quadrants (classes)\n",
        "quadrants = dataset[\"4Q\"].unique()"
      ],
      "metadata": {
        "id": "Uuz-uqp8H_Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a smaple midi file extract relevant information"
      ],
      "metadata": {
        "id": "Dcmf1t7lzy9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretty_midi"
      ],
      "metadata": {
        "id": "NU1ObemSzUCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pretty_midi\n",
        "\n",
        "pm = pretty_midi.PrettyMIDI(music_files[0])\n",
        "print('Number of instruments:', len(pm.instruments))\n",
        "instrument = pm.instruments[0]\n",
        "instrument_name = pretty_midi.program_to_instrument_name(instrument.program)\n",
        "print('Instrument name:', instrument_name)"
      ],
      "metadata": {
        "id": "i9JHugcd0JP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required methods"
      ],
      "metadata": {
        "id": "aRP9v-jK2yqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def midi_to_notes(midi_file: str,) -> pd.DataFrame:\n",
        "  pm = pretty_midi.PrettyMIDI(midi_file)\n",
        "  instrument = pm.instruments[0]\n",
        "  notes = collections.defaultdict(list)\n",
        "\n",
        "  # Sort the notes by start time\n",
        "  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n",
        "  prev_start = sorted_notes[0].start\n",
        "  for note in sorted_notes:\n",
        "    start = note.start\n",
        "    end = note.end\n",
        "    notes['pitch'].append(note.pitch)\n",
        "    notes['start'].append(start)\n",
        "    notes['end'].append(end)\n",
        "    notes['step'].append(start - prev_start)\n",
        "    notes['duration'].append(end - start)\n",
        "    prev_start = start\n",
        "\n",
        "  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})\n"
      ],
      "metadata": {
        "id": "vqYVB-ds246l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_notes(notes: pd.DataFrame, count: Optional[int] = None):\n",
        "  if count:\n",
        "    title = f'First {count} notes'\n",
        "  else:\n",
        "    title = f'Whole track'\n",
        "    count = len(notes['pitch'])\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)\n",
        "  plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)\n",
        "  plt.plot(\n",
        "      plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n",
        "  plt.xlabel('Time [s]')\n",
        "  plt.ylabel('Pitch')\n",
        "  _ = plt.title(title)\n"
      ],
      "metadata": {
        "id": "zZZUDECj3pX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5):\n",
        "  plt.figure(figsize=[15, 5])\n",
        "  plt.subplot(1, 3, 1)\n",
        "  sns.histplot(notes, x=\"pitch\", bins=20)\n",
        "\n",
        "  plt.subplot(1, 3, 2)\n",
        "  max_step = np.percentile(notes['step'], 100 - drop_percentile)\n",
        "  sns.histplot(notes, x=\"step\", bins=np.linspace(0, max_step, 21))\n",
        "\n",
        "  plt.subplot(1, 3, 3)\n",
        "  max_duration = np.percentile(notes['duration'], 100 - drop_percentile)\n",
        "  sns.histplot(notes, x=\"duration\", bins=np.linspace(0, max_duration, 21))\n"
      ],
      "metadata": {
        "id": "VpnUHTVC51Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def notes_to_midi(\n",
        "  notes: pd.DataFrame,\n",
        "  out_file: str, \n",
        "  instrument_name: str,\n",
        "  velocity: int = 100,  # note loudness\n",
        ") -> pretty_midi.PrettyMIDI:\n",
        "\n",
        "  pm = pretty_midi.PrettyMIDI()\n",
        "  instrument = pretty_midi.Instrument(\n",
        "      program=pretty_midi.instrument_name_to_program(\n",
        "          instrument_name))\n",
        "\n",
        "  prev_start = 0\n",
        "  for i, note in notes.iterrows():\n",
        "    start = float(prev_start + note['step'])\n",
        "    end = float(start + note['duration'])\n",
        "    note = pretty_midi.Note(\n",
        "        velocity=velocity,\n",
        "        pitch=int(note['pitch']),\n",
        "        start=start,\n",
        "        end=end,\n",
        "    )\n",
        "    instrument.notes.append(note)\n",
        "    prev_start = start\n",
        "\n",
        "  pm.instruments.append(instrument)\n",
        "  pm.write(out_file)\n",
        "  return pm\n"
      ],
      "metadata": {
        "id": "S3uUjHCZ93n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(\n",
        "    dataset: tf.data.Dataset, \n",
        "    seq_length: int,\n",
        "    vocab_size = 128,\n",
        ") -> tf.data.Dataset:\n",
        "  key_order = ['pitch', 'step', 'duration']\n",
        "  \"\"\"Returns TF Dataset of sequence and label examples.\"\"\"\n",
        "  seq_length = seq_length+1\n",
        "\n",
        "  # Take 1 extra for the labels\n",
        "  windows = dataset.window(seq_length, shift=1, stride=1,\n",
        "                              drop_remainder=True)\n",
        "\n",
        "  # `flat_map` flattens the\" dataset of datasets\" into a dataset of tensors\n",
        "  flatten = lambda x: x.batch(seq_length, drop_remainder=True)\n",
        "  sequences = windows.flat_map(flatten)\n",
        "\n",
        "  # Normalize note pitch\n",
        "  def scale_pitch(x):\n",
        "    x = x/[vocab_size,1.0,1.0]\n",
        "    return x\n",
        "\n",
        "  # Split the labels\n",
        "  def split_labels(sequences):\n",
        "    inputs = sequences[:-1]\n",
        "    labels_dense = sequences[-1]\n",
        "    labels = {key:labels_dense[i] for i,key in enumerate(key_order)}\n",
        "\n",
        "    return scale_pitch(inputs), labels\n",
        "\n",
        "  return sequences.map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "yIKkSF6TqsiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_note(\n",
        "    notes: np.ndarray, \n",
        "    keras_model: tf.keras.Model, \n",
        "    temperature: float = 1.0) -> int:\n",
        "  \"\"\"Generates a note IDs using a trained sequence model.\"\"\"\n",
        "\n",
        "  assert temperature > 0\n",
        "\n",
        "  # Add batch dimension\n",
        "  inputs = tf.expand_dims(notes, 0)\n",
        "\n",
        "  predictions = model.predict(inputs)\n",
        "  pitch_logits = predictions['pitch']\n",
        "  step = predictions['step']\n",
        "  duration = predictions['duration']\n",
        "\n",
        "  pitch_logits /= temperature\n",
        "  pitch = tf.random.categorical(pitch_logits, num_samples=1)\n",
        "  pitch = tf.squeeze(pitch, axis=-1)\n",
        "  duration = tf.squeeze(duration, axis=-1)\n",
        "  step = tf.squeeze(step, axis=-1)\n",
        "  # `step` and `duration` values should be non-negative\n",
        "  step = tf.maximum(0, step)\n",
        "  duration = tf.maximum(0, duration)\n",
        "\n",
        "  return int(pitch), float(step), float(duration)"
      ],
      "metadata": {
        "id": "4Geh5kHDC0qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_audio(pm: pretty_midi.PrettyMIDI, seconds=30):\n",
        "  _SAMPLING_RATE = 16000\n",
        "  waveform = pm.fluidsynth(fs=_SAMPLING_RATE)\n",
        "  # Take a sample of the generated waveform to mitigate kernel resets\n",
        "  waveform_short = waveform[:seconds*_SAMPLING_RATE]\n",
        "  return display.Audio(waveform_short, rate=_SAMPLING_RATE)"
      ],
      "metadata": {
        "id": "ONhHhzPWPXb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize data from files\n"
      ],
      "metadata": {
        "id": "O0aGbXn93IC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take one song from each class and plot the data \n",
        "song_ids = []\n",
        "for i in quadrants:\n",
        "  result = dataset.loc[dataset['4Q'] == i]\n",
        "  song_ids.append(result.iloc[0][\"ID\"])\n",
        "\n",
        "path = os.path.dirname(music_files[0])\n",
        "\n",
        "for value in song_ids :\n",
        "  raw_notes = midi_to_notes(f\"{path}/{value}.mid\")\n",
        "  plot_notes(raw_notes, count=100)\n",
        "  plot_distributions(raw_notes)\n"
      ],
      "metadata": {
        "id": "UrZ_xh1p3L8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Tensorflow dataset"
      ],
      "metadata": {
        "id": "lF-LhUpKjQ-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "notes_array = {}\n",
        "for files in music_files:\n",
        "  head, tail = os.path.split(files)\n",
        "  id = tail[:-4]\n",
        "  category = dataset.loc[dataset['ID'] == id]\n",
        "  category = category.iloc[0][\"4Q\"]\n",
        "  # notes = midi_to_notes(files)\n",
        "  if category in notes_array :\n",
        "    notes_array[category].append(files)\n",
        "  else:\n",
        "    notes_array[category] = [files]\n",
        "notes_array = OrderedDict(sorted(notes_array.items()))"
      ],
      "metadata": {
        "id": "7NTC7OXZjIuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# length of total notes in dataset for each class\n",
        "for key in notes_array:\n",
        "  print(len(notes_array[key]))"
      ],
      "metadata": {
        "id": "uiWKWRx8mhzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# take random 5 songs from each cat for training.\n",
        "training_notes_files = {}\n",
        "#n_notes = \n",
        "for key in notes_array:\n",
        "  choices = random.choices(notes_array[key],k=5)\n",
        "  training_notes_files[key] = choices\n",
        "training_notes = OrderedDict(sorted(training_notes_files.items()))\n",
        "\n",
        "training_notes = []\n",
        "for cat in training_notes_files :\n",
        "  files = training_notes_files[cat]\n",
        "  notes_total = []\n",
        "  for val in files :\n",
        "    notes = midi_to_notes(val)\n",
        "    notes_total.append(notes)\n",
        "  notes_total = pd.concat(notes_total)\n",
        "  training_notes.append(notes_total)\n"
      ],
      "metadata": {
        "id": "cetE7PK3SyCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_notes = []\n",
        "key_order = ['pitch', 'step', 'duration']\n",
        "for notes_for_class in training_notes:\n",
        "  music_data = np.stack([notes_for_class[key] for key in key_order], axis=1)\n",
        "  train_notes.append(music_data)"
      ],
      "metadata": {
        "id": "TJamgE3Wm8Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "note_lens = []\n",
        "for note_len in train_notes:\n",
        "  note_lens.append(len(note_len))\n",
        "print(note_lens)"
      ],
      "metadata": {
        "id": "8OCS_5XHna10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = []\n",
        "\n",
        "for i in train_notes :\n",
        "  notes_ds = tf.data.Dataset.from_tensor_slices(i)\n",
        "  print(notes_ds.element_spec)\n",
        "  seq_length = 25\n",
        "  vocab_size = 128\n",
        "  seq_ds = create_sequences(notes_ds, seq_length, vocab_size)\n",
        "  print(seq_ds.element_spec)\n",
        "  ds.append(seq_ds)"
      ],
      "metadata": {
        "id": "rlrzC1MiqmuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_ds in ds :\n",
        "  for seq, target in seq_ds.take(1):\n",
        "    print('sequence shape:', seq.shape)\n",
        "    print('sequence elements (first 10):', seq[0: 10])\n",
        "    print()\n",
        "    print('target:', target)"
      ],
      "metadata": {
        "id": "BXFBEU_GraE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = []\n",
        "for i in range(len(ds)) :\n",
        "  seq_ds = ds[i]\n",
        "  batch_size = 10\n",
        "  buffer_size = note_lens[i] - seq_length  # the number of items in the dataset\n",
        "  train_ds = (seq_ds\n",
        "            .shuffle(buffer_size)\n",
        "            .batch(batch_size, drop_remainder=True)\n",
        "            .cache()\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "  train_dataset.append(train_ds)"
      ],
      "metadata": {
        "id": "JQlaRvF7rfgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "chM54n7_r1n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
        "  mse = (y_true - y_pred) ** 2\n",
        "  positive_pressure = 10 * tf.maximum(-y_pred, 0.0)\n",
        "  return tf.reduce_mean(mse + positive_pressure)"
      ],
      "metadata": {
        "id": "31xHEfDer1Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (seq_length, 3)\n",
        "learning_rate = 0.005\n",
        "\n",
        "inputs = tf.keras.Input(input_shape)\n",
        "x = tf.keras.layers.LSTM(128)(inputs)\n",
        "\n",
        "outputs = {\n",
        "  'pitch': tf.keras.layers.Dense(128, name='pitch')(x),\n",
        "  'step': tf.keras.layers.Dense(1, name='step')(x),\n",
        "  'duration': tf.keras.layers.Dense(1, name='duration')(x),\n",
        "}\n",
        "\n",
        "loss = {\n",
        "        'pitch': tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True),\n",
        "        'step': mse_with_positive_pressure,\n",
        "        'duration': mse_with_positive_pressure\n",
        "  }\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# create 4 models for different emotions\n",
        "models = []\n",
        "\n",
        "for i in range(len(train_dataset)):\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  models.append(model)"
      ],
      "metadata": {
        "id": "V62HJmhvr5ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in models:\n",
        "  i.compile(loss=loss, optimizer=optimizer,run_eagerly=True)\n",
        "  print(i.summary())"
      ],
      "metadata": {
        "id": "4D_23vKJs9MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(models)):\n",
        "  losses = model.evaluate(train_dataset[i], return_dict=True)\n",
        "  print(losses)"
      ],
      "metadata": {
        "id": "s5LkldPIwOze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(models)):\n",
        "  models[i].compile(\n",
        "      loss=loss,\n",
        "      loss_weights={\n",
        "          'pitch': 0.05,\n",
        "          'step': 1.0,\n",
        "          'duration':1.0\n",
        "      },\n",
        "      optimizer=optimizer,\n",
        "  )\n",
        "  losses = models[i].evaluate(train_dataset[i], return_dict=True)\n",
        "  print(losses)"
      ],
      "metadata": {
        "id": "gNBgB-MotN53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='loss',\n",
        "        patience=5,\n",
        "        verbose=1,\n",
        "        restore_best_weights=True),\n",
        "]"
      ],
      "metadata": {
        "id": "0j8ylkDNuT4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "histories = []\n",
        "epochs = 50\n",
        "\n",
        "for i in range(len(models)) :\n",
        "\n",
        "  history = models[i].fit(\n",
        "    train_dataset[i],\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "  )\n",
        "  histories.append(history)"
      ],
      "metadata": {
        "id": "6iijJNjIv4do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for history in histories :\n",
        "  plt.plot(history.epoch, history.history['loss'], label='total loss')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "RNAWLZPLAGrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_music(model_type):\n",
        "  temperature = 2.0\n",
        "  num_predictions = 200\n",
        "\n",
        "  sample_notes = np.stack([raw_notes[key] for key in key_order], axis=1)\n",
        "\n",
        "  # The initial sequence of notes; pitch is normalized similar to training\n",
        "  # sequences\n",
        "  input_notes = (\n",
        "      sample_notes[:seq_length] / np.array([vocab_size, 1, 1]))\n",
        "\n",
        "  generated_notes = []\n",
        "  prev_start = 0\n",
        "  for _ in range(num_predictions):\n",
        "    pitch, step, duration = predict_next_note(input_notes, model_type, temperature)\n",
        "    start = prev_start + step\n",
        "    end = start + duration\n",
        "    input_note = (pitch, step, duration)\n",
        "    \n",
        "    generated_notes.append((*input_note, start, end))\n",
        "    input_notes = np.delete(input_notes, 0, axis=0)\n",
        "    input_notes = np.append(input_notes, np.expand_dims(input_note, 0), axis=0)\n",
        "    prev_start = start\n",
        "\n",
        "  generated_notes = pd.DataFrame(\n",
        "      generated_notes, columns=(*key_order, 'start', 'end'))\n",
        "  return generated_notes"
      ],
      "metadata": {
        "id": "an7_Enm_DqWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using text to emotion to map to a model"
      ],
      "metadata": {
        "id": "zIdO0m2tmGeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install text2emotion\n",
        "!pip install emoji~=1.6.3\n",
        "import text2emotion as te"
      ],
      "metadata": {
        "id": "pb826R1MmB9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "id": "9PRFVvCjmLSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = te.get_emotion(\"angry\")\n",
        "emotion = max(result,key=result.get)\n",
        "index = 0\n",
        "if(emotion == \"Happy\"):\n",
        "  index = 0\n",
        "elif (emotion == \"Angry\"):\n",
        "  index = 1\n",
        "elif (emotion == \"Fear\"):\n",
        "  index = 1\n",
        "elif (emotion == \"Sad\"):\n",
        "  index = 3\n",
        "else :\n",
        "  index = 2"
      ],
      "metadata": {
        "id": "F-DE4RNMoWrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index,emotion)"
      ],
      "metadata": {
        "id": "zC7Ni1IZroeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_notes = generate_music(models[index])"
      ],
      "metadata": {
        "id": "0hhmgoG5pbsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_file = 'output.mid'\n",
        "out_pm = notes_to_midi(\n",
        "    generated_notes, out_file=out_file, instrument_name=instrument_name)"
      ],
      "metadata": {
        "id": "N8tWa3-HoFPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the models\n"
      ],
      "metadata": {
        "id": "b2z1ZCxhxIfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(models)) :\n",
        "  model.save(f'./{i}')\n"
      ],
      "metadata": {
        "id": "niYEZ-2GxHtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!mkdir /content/gdrive/My\\ Drive/Colab_Models"
      ],
      "metadata": {
        "id": "2QAPaSigaUi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(models)) :\n",
        "  model.save(f'/content/gdrive/My Drive/New_Colab_Models/{i}')"
      ],
      "metadata": {
        "id": "cyxmlLJ4Z9M2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}